import pandas as pd
import streamlit as st
import subprocess
import json
import os
import time
import glob
import jieba
import pickle
from pyecharts.charts import Bar, Pie, Scatter, HeatMap, WordCloud
from pyecharts import options as opts
from streamlit_echarts import st_pyecharts
from collections import defaultdict
from bs4 import BeautifulSoup

# å®šä¹‰çˆ¬è™«å‘½ä»¤é€‰é¡¹
# æ·»åŠ æ›´å¯é çš„JavaScriptç›‘æµ‹ä»£ç 

crawler_commands = {
    "çˆ¬å–ç”¨æˆ·ä¿¡æ¯": {
        "func": 1,
        "file_template": "D:\\scrapy_zhihu\\data\\{user_name}.json"
    },
    "çˆ¬å–å…³æ³¨è€…ä¿¡æ¯": {
        "func": 2,
        "file_template": "D:\\scrapy_zhihu\\data\\{user_name}_followees.json"
    },
    "çˆ¬å–ç²‰ä¸ä¿¡æ¯": {
        "func": 3,
        "file_template": "D:\\scrapy_zhihu\\data\\{user_name}_followers.json"
    },
    "çˆ¬å–å›ç­”åŠè¯„è®ºä¿¡æ¯": {
        "func": 4,
        "file_template": "D:\\scrapy_zhihu\\data\\{user_name}_answers\\1.json"
    },
    "çˆ¬å–æé—®åŠå›ç­”ä¿¡æ¯": {
        "func": 5,
        "file_template": "D:\\scrapy_zhihu\\data\\{user_name}_questions\\1.json"
    },
    "é‡æ–°çˆ¬å–æ‰€æœ‰ä¿¡æ¯": {
        "func": 6,
        "file_template": None  # ç‰¹æ®Šå¤„ç†ï¼Œéœ€è¦æ£€æŸ¥å¤šä¸ªæ–‡ä»¶
    },
    "è‡ªåŠ¨æ›´æ–°å›ç­”å’Œæé—®ä¿¡æ¯": {
        "func": 7,
        "file_template": None  # ç‰¹æ®Šå¤„ç†ï¼Œéœ€è¦æ£€æŸ¥å¤šä¸ªæ–‡ä»¶
    }
}
def get_file_path(user_name, func_value):
    """æ ¹æ®åŠŸèƒ½å·è·å–å¯¹åº”çš„æ–‡ä»¶è·¯å¾„"""
    for cmd in crawler_commands.values():
        if cmd["func"] == func_value:
            if cmd["file_template"]:
                return cmd["file_template"].format(user_name=user_name)
    return None

def check_file_update(file_path, last_modified_time=None):
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ›´æ–°
    :param file_path: æ–‡ä»¶è·¯å¾„
    :param last_modified_time: ä¸Šæ¬¡ä¿®æ”¹æ—¶é—´
    :return: (æ˜¯å¦æ›´æ–°, å½“å‰ä¿®æ”¹æ—¶é—´, æ–‡ä»¶å†…å®¹)
    """
    if not os.path.exists(file_path):
        return False, None, None
    
    current_mtime = os.path.getmtime(file_path)
    
    if last_modified_time is None or current_mtime > last_modified_time:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = json.load(f)
            return True, current_mtime, content
        except Exception as e:
            st.error(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            return False, current_mtime, None
    return False, current_mtime, None
def analyze_data(user_name):
    # åŠ è½½æ•°æ®è·¯å¾„
    data_paths = {
        "followers": f"D:\\scrapy_zhihu\\data\\{user_name}_followers.json",
        "followees": f"D:\\scrapy_zhihu\\data\\{user_name}_followees.json",
        "answers": f"D:\\scrapy_zhihu\\data\\{user_name}_answers",
        "questions": f"D:\\scrapy_zhihu\\data\\{user_name}_questions"
    }
    
    # åŠ è½½åœç”¨è¯
    stopwords = set()
    if os.path.exists('./stopwords.txt'):
        with open('./stopwords.txt', 'r', encoding='utf-8') as f:
            stopwords = set(f.read().splitlines())
    
    # åŠ è½½æ•°æ®
    def load_user_data(file_path):
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    followers = load_user_data(data_paths["followers"])
    followees = load_user_data(data_paths["followees"])
    
    # æ•°æ®å¤„ç†å‡½æ•°
    def process_data(user_list):
        return {
            'gender': [u.get('gender', -1) for u in user_list],
            'names': [u.get('name', '') for u in user_list],
            'headlines': [u.get('headline', '') for u in user_list],
            'answer_counts': [u.get('answer_count', 0) for u in user_list],
            'follower_counts': [u.get('follower_count', 0) for u in user_list]
        }
    
    followers_data = process_data(followers)
    followees_data = process_data(followees)
    # å¯è§†åŒ–åˆ†æ
    st.header(f"{user_name}çš„æ•°æ®åˆ†æç»“æœ")
    
    # åˆ›å»ºé€‰é¡¹å¡åˆ†å¼€æ˜¾ç¤ºå…³æ³¨è€…å’Œç²‰ä¸åˆ†æ
    tab1, tab2, tab3, tab4 = st.tabs(["ç²‰ä¸åˆ†æ", "å…³æ³¨è€…åˆ†æ", "ç”¨æˆ·å›ç­”åˆ†æ", "ç”¨æˆ·æé—®åˆ†æ"])
    
    with tab1:
        st.warning("å¦‚æœå›¾è¡¨æœªåŠ è½½ï¼Œè¯·åˆ·æ–°é¡µé¢æˆ–å†æ¬¡ç‚¹å‡»åˆ†ææ•°æ®æŒ‰é’®ã€‚")
        st.subheader("ç²‰ä¸æ•°æ®åˆ†æ")
        if followers:
            # 1. ç²‰ä¸æ€§åˆ«åˆ†å¸ƒ
            st.markdown("### 1. ç²‰ä¸æ€§åˆ«åˆ†å¸ƒ")
            gender_data = [
                ['ç”·æ€§', followers_data['gender'].count(1)],
                ['å¥³æ€§', followers_data['gender'].count(0)],
                ['æœªçŸ¥', followers_data['gender'].count(-1)]
            ]
            
            col1, col2 = st.columns(2)
            with col1:
                pie = (
                    Pie(init_opts=opts.InitOpts(width="100%", height="400px"))
                    .add("", gender_data)
                    .set_global_opts(title_opts=opts.TitleOpts(title="ç²‰ä¸æ€§åˆ«åˆ†å¸ƒ",padding=[20, 0, 0, 110]))
                    .set_series_opts(label_opts=opts.LabelOpts(formatter="{b}: {c} ({d}%)"))
                )
                st_pyecharts(pie, height=400)
            
            with col2:
                bar = (
                    Bar(init_opts=opts.InitOpts(width="100%", height="400px"))
                    .add_xaxis([x[0] for x in gender_data])
                    .add_yaxis("æ•°é‡", [x[1] for x in gender_data])
                    .set_global_opts(title_opts=opts.TitleOpts(title="ç²‰ä¸æ€§åˆ«åˆ†å¸ƒæŸ±çŠ¶å›¾",padding=[20, 0, 0, 90]))
                )
                st_pyecharts(bar, height=400)
            
            # 2. ç²‰ä¸çš„å›ç­”æ•°é‡åˆ†å¸ƒ
            st.markdown("### 2. ç²‰ä¸çš„å›ç­”æ•°é‡")
            answer_counts = followers_data['answer_counts']
            
            groups = {
                '0': answer_counts.count(0),
                '1-20': len([x for x in answer_counts if 1 <= x <= 20]),
                '21-50': len([x for x in answer_counts if 21 <= x <= 50]),
                '51-100': len([x for x in answer_counts if 51 <= x <= 100]),
                '>100': len([x for x in answer_counts if x > 100])
            }
            
            bar = (
                Bar(init_opts=opts.InitOpts(width="100%", height="400px"))
                .add_xaxis(list(groups.keys()))
                .add_yaxis("äººæ•°", list(groups.values()))
                .set_global_opts(title_opts=opts.TitleOpts(title="ç²‰ä¸å›ç­”æ•°é‡åˆ†å¸ƒ"))
            )
            st_pyecharts(bar, height=400)
            
            # 3. ç²‰ä¸çš„ç²‰ä¸æ•°é‡åˆ†å¸ƒ
            st.markdown("### 3. ç²‰ä¸çš„ç²‰ä¸æ•°é‡")
            follower_counts = followers_data['follower_counts']
            
            scatter_data = [[str(i), count] for i, count in enumerate(follower_counts) if count > 0]
            scatter = (
                Scatter(init_opts=opts.InitOpts(width="100%", height="400px"))
                .add_xaxis([x[0] for x in scatter_data])
                .add_yaxis("ç²‰ä¸æ•°", [x[1] for x in scatter_data])
                .set_global_opts(
                    title_opts=opts.TitleOpts(title="ç²‰ä¸çš„ç²‰ä¸æ•°é‡åˆ†å¸ƒ"),
                    visualmap_opts=opts.VisualMapOpts(max_=max(follower_counts) if follower_counts else None)
                )
            )
            st_pyecharts(scatter, height=400)
            
            # 4. ç²‰ä¸ä¸ªäººç®€ä»‹å…³é”®è¯
            st.markdown("### 4. ç²‰ä¸ä¸ªäººç®€ä»‹å…³é”®è¯")
            headlines = [h for h in followers_data['headlines'] if h]
            
            word_freq = defaultdict(int)
            for text in headlines:
                words = jieba.cut(text)
                for word in words:
                    if word not in stopwords and len(word) > 1:
                        word_freq[word] += 1
            
            if word_freq:
                wordcloud_data = [[k, v] for k, v in sorted(word_freq.items(), key=lambda x: -x[1])[:50]]
                wordcloud = (
                    WordCloud(init_opts=opts.InitOpts(width="100%", height="400px"))
                    .add("", wordcloud_data, word_size_range=[10, 60])
                    .set_global_opts(title_opts=opts.TitleOpts(title="ç²‰ä¸ä¸ªäººç®€ä»‹å…³é”®è¯"))
                )
                st_pyecharts(wordcloud, height=400)
            else:
                st.warning("æ²¡æœ‰å¯ç”¨çš„ä¸ªäººç®€ä»‹æ•°æ®")
        else:
            st.warning("æ²¡æœ‰ç²‰ä¸æ•°æ®å¯ä¾›åˆ†æ")

    # åœ¨å…³æ³¨è€…åˆ†æé€‰é¡¹å¡ä¸­åŒæ ·ä¿®æ”¹
    with tab2:
        st.warning("å¦‚æœå›¾è¡¨æœªåŠ è½½ï¼Œè¯·åˆ·æ–°é¡µé¢æˆ–å†æ¬¡ç‚¹å‡»åˆ†ææ•°æ®æŒ‰é’®ã€‚")
        st.subheader("å…³æ³¨è€…æ•°æ®åˆ†æ")
        if followees:
            # 1. å…³æ³¨è€…æ€§åˆ«åˆ†å¸ƒ
            st.markdown("### 1. å…³æ³¨è€…æ€§åˆ«åˆ†å¸ƒ")
            gender_data = [
                ['ç”·æ€§', followees_data['gender'].count(1)],
                ['å¥³æ€§', followees_data['gender'].count(0)],
                ['æœªçŸ¥', followees_data['gender'].count(-1)]
            ]
            
            col1, col2 = st.columns(2)
            with col1:
                pie = (
                    Pie(init_opts=opts.InitOpts(width="100%", height="400px"))
                    .add("", gender_data)
                    .set_global_opts(title_opts=opts.TitleOpts(title="å…³æ³¨è€…æ€§åˆ«åˆ†å¸ƒ",padding=[20, 0, 0, 110]))
                    .set_series_opts(label_opts=opts.LabelOpts(formatter="{b}: {c} ({d}%)"))
                )
                st_pyecharts(pie, height=400)
            
            with col2:
                bar = (
                    Bar(init_opts=opts.InitOpts(width="100%", height="400px"))
                    .add_xaxis([x[0] for x in gender_data])
                    .add_yaxis("æ•°é‡", [x[1] for x in gender_data])
                    .set_global_opts(title_opts=opts.TitleOpts(title="å…³æ³¨è€…æ€§åˆ«åˆ†å¸ƒæŸ±çŠ¶å›¾",padding=[20, 0, 0, 90]))
                )
                st_pyecharts(bar, height=400)
            # 2. å…³æ³¨è€…çš„å›ç­”æ•°é‡åˆ†å¸ƒ
            st.markdown("### 2. å…³æ³¨è€…çš„å›ç­”æ•°é‡")
            answer_counts = followees_data['answer_counts']
            
            groups = {
                '0': answer_counts.count(0),
                '1-20': len([x for x in answer_counts if 1 <= x <= 20]),
                '21-50': len([x for x in answer_counts if 21 <= x <= 50]),
                '51-100': len([x for x in answer_counts if 51 <= x <= 100]),
                '>100': len([x for x in answer_counts if x > 100])
            }
        
            bar = (
                Bar(init_opts=opts.InitOpts(width="100%", height="400px"))
                .add_xaxis(list(groups.keys()))
                .add_yaxis("äººæ•°", list(groups.values()))
                .set_global_opts(title_opts=opts.TitleOpts(title="å…³æ³¨è€…å›ç­”æ•°é‡åˆ†å¸ƒ"))
            )
            st_pyecharts(bar, height=400)
            # 3. å…³æ³¨è€…çš„ç²‰ä¸æ•°é‡åˆ†å¸ƒ
            st.markdown("### 3. å…³æ³¨è€…çš„ç²‰ä¸æ•°é‡")
            follower_counts = followees_data['follower_counts']
            
            scatter_data = [[str(i), count] for i, count in enumerate(follower_counts) if count > 0]
            scatter = (
                Scatter(init_opts=opts.InitOpts(width="100%", height="400px"))
                .add_xaxis([x[0] for x in scatter_data])
                .add_yaxis("ç²‰ä¸æ•°", [x[1] for x in scatter_data])
                .set_global_opts(
                    title_opts=opts.TitleOpts(title="å…³æ³¨è€…çš„ç²‰ä¸æ•°é‡åˆ†å¸ƒ"),
                    visualmap_opts=opts.VisualMapOpts(max_=max(follower_counts) if follower_counts else None)
                )
            )
            st_pyecharts(scatter, height=400)
           
            # 4. å…³æ³¨è€…ä¸ªäººç®€ä»‹å…³é”®è¯
            st.markdown("### 4. å…³æ³¨è€…ä¸ªäººç®€ä»‹å…³é”®è¯")
            headlines = [h for h in followees_data['headlines'] if h]
            
            word_freq = defaultdict(int)
            for text in headlines:
                words = jieba.cut(text)
                for word in words:
                    if word not in stopwords and len(word) > 1:
                        word_freq[word] += 1
            
            if word_freq:
                wordcloud_data = [[k, v] for k, v in sorted(word_freq.items(), key=lambda x: -x[1])[:50]]
                wordcloud = (
                    WordCloud()
                    .add("", wordcloud_data, word_size_range=[10, 60])
                    .set_global_opts(title_opts=opts.TitleOpts(title="å…³æ³¨è€…ä¸ªäººç®€ä»‹å…³é”®è¯"))
                )
                st_pyecharts(wordcloud)
            else:
                st.warning("æ²¡æœ‰å¯ç”¨çš„ä¸ªäººç®€ä»‹æ•°æ®")
        else:
            st.warning("æ²¡æœ‰å…³æ³¨è€…æ•°æ®å¯ä¾›åˆ†æ")
    with tab3:
        st.warning("å¦‚æœå›¾è¡¨æœªåŠ è½½ï¼Œè¯·åˆ·æ–°é¡µé¢æˆ–å†æ¬¡ç‚¹å‡»åˆ†ææ•°æ®æŒ‰é’®ã€‚")
        st.subheader("ç”¨æˆ·å›ç­”åŠè¯„è®ºè¯äº‘åˆ†æ")
        
        # å®šä¹‰ç”¨æˆ·å›ç­”æ•°æ®è·¯å¾„
        answers_dir = data_paths["answers"]
        
        if os.path.exists(answers_dir):
            # è·å–æœ€å¤š10ä¸ªå›ç­”æ–‡ä»¶
            answer_files = sorted(os.listdir(answers_dir))[:10]
            all_text = ""
            
            # éå†æ¯ä¸ªå›ç­”æ–‡ä»¶
            for file in answer_files:
                file_path = os.path.join(answers_dir, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        answer_data = json.load(f)
                        
                        # æå–é—®é¢˜æ ‡é¢˜
                        all_text += answer_data.get('question', '') + " "
                        
                        # æå–å›ç­”å†…å®¹ï¼ˆå»é™¤HTMLæ ‡ç­¾ï¼‰
                        content = answer_data.get('content', '')
                        if content:
                            soup = BeautifulSoup(content, 'html.parser')
                            all_text += soup.get_text() + " "
                        
                        # æå–è¯„è®ºå†…å®¹
                        comments = answer_data.get('answer_comment', [])
                        for comment in comments:
                            comment_content = comment.get('comment_content', '')
                            if comment_content:
                                soup = BeautifulSoup(comment_content, 'html.parser')
                                all_text += soup.get_text() + " "
                
                except Exception as e:
                    st.warning(f"è¯»å–æ–‡ä»¶ {file} æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            if all_text.strip():
                # ä¸­æ–‡åˆ†è¯å¤„ç†
                word_freq = defaultdict(int)
                words = jieba.cut(all_text)
                for word in words:
                    if word not in stopwords and len(word) > 1:  # è¿‡æ»¤åœç”¨è¯å’Œå•å­—
                        word_freq[word] += 1
                
                if word_freq:
                    # å–å‰50ä¸ªé«˜é¢‘è¯
                    wordcloud_data = [[k, v] for k, v in sorted(word_freq.items(), key=lambda x: -x[1])[:50]]
                    
                    # åˆ›å»ºè¯äº‘
                    wordcloud = (
                        WordCloud(init_opts=opts.InitOpts(width="100%", height="500px"))
                        .add("", wordcloud_data, word_size_range=[15, 80], shape='circle')
                        .set_global_opts(
                            title_opts=opts.TitleOpts(
                                title="ç”¨æˆ·å›ç­”åŠè¯„è®ºå…³é”®è¯åˆ†æ",
                                pos_top="5%",
                                padding=[0, 0, 20, 0]
                            ),
                            tooltip_opts=opts.TooltipOpts(is_show=True)
                        )
                    )
                    st_pyecharts(wordcloud, height=500)
                    # æ·»åŠ é«˜é¢‘è¯è¡¨æ ¼å±•ç¤º
                    st.markdown("### é«˜é¢‘è¯ç»Ÿè®¡")
                    df_words = pd.DataFrame(sorted(word_freq.items(), key=lambda x: -x[1])[:20], 
                                        columns=["å…³é”®è¯", "å‡ºç°æ¬¡æ•°"])
                    st.dataframe(df_words)
                else:
                    st.warning("æœªèƒ½æå–æœ‰æ•ˆå…³é”®è¯")
            else:
                st.warning("æ²¡æœ‰å¯åˆ†æçš„æ–‡æœ¬å†…å®¹")
        else:
            st.warning(f"æœªæ‰¾åˆ°ç”¨æˆ·å›ç­”ç›®å½•: {answers_dir}")
    with tab4:
        st.warning("å¦‚æœå›¾è¡¨æœªåŠ è½½ï¼Œè¯·åˆ·æ–°é¡µé¢æˆ–å†æ¬¡ç‚¹å‡»åˆ†ææ•°æ®æŒ‰é’®ã€‚")
        st.subheader("ç”¨æˆ·æé—®åŠå›ç­”è¯äº‘åˆ†æ")
        
        # å®šä¹‰ç”¨æˆ·æé—®æ•°æ®è·¯å¾„
        questions_dir = f"D:\\scrapy_zhihu\\data\\{user_name}_questions"
        
        if os.path.exists(questions_dir):
            # è·å–æœ€å¤š10ä¸ªé—®é¢˜æ–‡ä»¶
            question_files = sorted(os.listdir(questions_dir))[:10]
            all_text = ""
            
            # éå†æ¯ä¸ªé—®é¢˜æ–‡ä»¶
            for file in question_files:
                file_path = os.path.join(questions_dir, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        question_data = json.load(f)
                        
                        # æå–é—®é¢˜æ ‡é¢˜
                        all_text += question_data.get('question', '') + " "
                        
                        # æå–é—®é¢˜å›ç­”å†…å®¹
                        answers = question_data.get('question_answer', [])
                        for answer in answers:
                            # æå–å›ç­”å†…å®¹ï¼ˆå»é™¤HTMLæ ‡ç­¾ï¼‰
                            answer_content = answer.get('answer_content', '')
                            if answer_content:
                                soup = BeautifulSoup(answer_content, 'html.parser')
                                all_text += soup.get_text() + " "
                
                except Exception as e:
                    st.warning(f"è¯»å–æ–‡ä»¶ {file} æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            if all_text.strip():
                # ä¸­æ–‡åˆ†è¯å¤„ç†
                word_freq = defaultdict(int)
                words = jieba.cut(all_text)
                for word in words:
                    if word not in stopwords and len(word) > 1:  # è¿‡æ»¤åœç”¨è¯å’Œå•å­—
                        word_freq[word] += 1
                
                if word_freq:
                    # å–å‰50ä¸ªé«˜é¢‘è¯
                    wordcloud_data = [[k, v] for k, v in sorted(word_freq.items(), key=lambda x: -x[1])[:50]]
                    
                    # åˆ›å»ºè¯äº‘
                    wordcloud = (
                        WordCloud(init_opts=opts.InitOpts(width="100%", height="500px"))
                        .add("", wordcloud_data, word_size_range=[15, 80], shape='circle')
                        .set_global_opts(
                            title_opts=opts.TitleOpts(
                                title="ç”¨æˆ·æé—®åŠå›ç­”å…³é”®è¯åˆ†æ",
                                pos_top="5%",
                                padding=[0, 0, 20, 0]
                            ),
                            tooltip_opts=opts.TooltipOpts(is_show=True)
                        )
                    )
                    st_pyecharts(wordcloud, height=500)
                    
                    # æ·»åŠ é«˜é¢‘è¯è¡¨æ ¼å±•ç¤º
                    st.markdown("### é«˜é¢‘è¯ç»Ÿè®¡")
                    df_words = pd.DataFrame(sorted(word_freq.items(), key=lambda x: -x[1])[:20], 
                                        columns=["å…³é”®è¯", "å‡ºç°æ¬¡æ•°"])
                    st.dataframe(df_words)
                else:
                    st.warning("æœªèƒ½æå–æœ‰æ•ˆå…³é”®è¯")
            else:
                st.warning("æ²¡æœ‰å¯åˆ†æçš„æ–‡æœ¬å†…å®¹")
        else:
            st.warning(f"æœªæ‰¾åˆ°ç”¨æˆ·æé—®ç›®å½•: {questions_dir}")
st.sidebar.title("å¯¼èˆª")
page = st.sidebar.radio("é€‰æ‹©é¡µé¢", ["æ•°æ®çˆ¬å–", "æ•°æ®åˆ†æ"])
if page == "æ•°æ®çˆ¬å–":
# Streamlit åº”ç”¨æ ‡é¢˜
    st.title("çŸ¥ä¹çˆ¬è™«æ§åˆ¶é¢æ¿")
    st.warning("ç¬¬ä¸€æ¬¡ç™»å½•çˆ¬è™«è¯·å…ˆåœ¨å‘½ä»¤è¡Œæ‰§è¡Œ scrapy crawl zhihu å‘½ä»¤ï¼Œè¿›å…¥ç½‘é¡µåä¼šæœ‰ç™»å½•æç¤ºï¼Œä½¿ç”¨æ‰‹åŠ¨æ‰«ç /å¯†ç ç™»å½• ç™»å½•æˆåŠŸåï¼Œå›åˆ°å‘½ä»¤è¡Œçš„è¾“å‡ºåŒºè¾“å…¥ä»»æ„å­—ç¬¦")
    # è¾“å…¥ç”¨æˆ·å
    user_name = st.text_input("è¾“å…¥ç”¨æˆ·å", "å»–é›ªå³°")

    # é€‰æ‹©çˆ¬è™«å‘½ä»¤
    selected_command = st.selectbox("é€‰æ‹©çˆ¬è™«å‘½ä»¤", list(crawler_commands.keys()))

    # æ˜¾ç¤ºé€‰æ‹©çš„å‘½ä»¤
    st.write(f"ä½ é€‰æ‹©çš„å‘½ä»¤æ˜¯: {selected_command}")

    # æ‰§è¡Œçˆ¬è™«å‘½ä»¤
    if st.button("æ‰§è¡Œçˆ¬è™«"):
        # å¯åŠ¨ Chrome æµè§ˆå™¨
        chrome_data_dir = "D:\\scrapy_zhihu\\chrome_data"  # ä¿®æ”¹ä¸ºæ›´ç®€å•çš„è·¯å¾„
        os.makedirs(chrome_data_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨

        chrome_command = [
            "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe",
            "--remote-debugging-port=9222",
            f"--user-data-dir={chrome_data_dir}",  # å»æ‰è·¯å¾„çš„å¼•å·
            "--no-first-run",
            "--headless",
            "--no-default-browser-check",
            "--disable-popup-blocking"
        ]
        
        try:
            # å¯åŠ¨ Chrome æµè§ˆå™¨
            chrome_process = subprocess.Popen(chrome_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            st.success("Chrome æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼")
            
            # ç­‰å¾… Chrome å¯åŠ¨å®Œæˆ
            time.sleep(1)  # ç­‰å¾… 5 ç§’ï¼Œç¡®ä¿ Chrome å®Œå…¨å¯åŠ¨
            
            # è·å–å¯¹åº”çš„ Scrapy å‘½ä»¤å‚æ•°
            func_value = crawler_commands[selected_command]["func"]
            file_template = crawler_commands[selected_command]["file_template"]
            
            if func_value == 6:  # é‡æ–°çˆ¬å–æ‰€æœ‰ä¿¡æ¯ - æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶
                files_to_check = [
                    get_file_path(user_name, 1),  # ç”¨æˆ·ä¿¡æ¯
                    get_file_path(user_name, 2),  # å…³æ³¨è€…
                    get_file_path(user_name, 3),  # ç²‰ä¸
                    get_file_path(user_name, 4),  # å›ç­”
                    get_file_path(user_name, 5)   # æé—®
                ]
                last_times = [os.path.getmtime(f) if f and os.path.exists(f) else None for f in files_to_check]
            elif func_value == 7:  # è‡ªåŠ¨æ›´æ–°å›ç­”å’Œæé—®ä¿¡æ¯ - åªæ£€æŸ¥å›ç­”å’Œæé—®
                files_to_check = [
                    get_file_path(user_name, 4),  # å›ç­”
                    get_file_path(user_name, 5)   # æé—®
                ]
                last_times = [os.path.getmtime(f) if f and os.path.exists(f) else None for f in files_to_check]
            else:  # å…¶ä»–åŠŸèƒ½åªæ£€æŸ¥å¯¹åº”çš„å•ä¸ªæ–‡ä»¶
                files_to_check = [get_file_path(user_name, func_value)]
                last_times = [os.path.getmtime(files_to_check[0]) if files_to_check[0] and os.path.exists(files_to_check[0]) else None]
                
                # æ‰§è¡Œçˆ¬è™«å‘½ä»¤
            scrapy_command = [
                    "scrapy", "crawl", "zhihu",
                    "-a", f"user_name={user_name}",
                    "-a", f"func={func_value}"
                ]
                
            try:
                    result = subprocess.run(scrapy_command, capture_output=True, text=True)
                    
                    if result.returncode == 0:
                        time.sleep(1)  # ç­‰å¾…æ–‡ä»¶å†™å…¥
                        
                        updated_files = []
                        
                        # åŠŸèƒ½4ï¼šå±•ç¤ºansweræ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶
                        if func_value == 4:
                            answer_dir = os.path.join("D:\\scrapy_zhihu\\data", f"{user_name}_answers")
                            if os.path.exists(answer_dir):
                                answer_files = glob.glob(os.path.join(answer_dir, "*.json"))
                                for file_path in answer_files:
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        content = json.load(f)
                                    updated_files.append((file_path, content))
                        
                        # åŠŸèƒ½5ï¼šå±•ç¤ºquestionæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶
                        elif func_value == 5:
                            question_dir = os.path.join("D:\\scrapy_zhihu\\data", f"{user_name}_questions")
                            if os.path.exists(question_dir):
                                question_files = glob.glob(os.path.join(question_dir, "*.json"))
                                for file_path in question_files:
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        content = json.load(f)
                                    updated_files.append((file_path, content))
                        
                        # åŠŸèƒ½6ï¼šåŒæ—¶å±•ç¤ºanswerå’Œquestionæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶
                        elif func_value == 6:
                            # æ£€æŸ¥answeræ–‡ä»¶å¤¹
                            user_file = get_file_path(user_name, 1)
                            if user_file and os.path.exists(user_file):
                                with open(user_file, 'r', encoding='utf-8') as f:
                                    content = json.load(f)
                                updated_files.append((user_file, content))
                            
                            # æ£€æŸ¥å…³æ³¨è€…æ–‡ä»¶
                            followee_file = get_file_path(user_name, 2)
                            if followee_file and os.path.exists(followee_file):
                                with open(followee_file, 'r', encoding='utf-8') as f:
                                    content = json.load(f)
                                updated_files.append((followee_file, content))
                            
                            # æ£€æŸ¥ç²‰ä¸æ–‡ä»¶
                            follower_file = get_file_path(user_name, 3)
                            if follower_file and os.path.exists(follower_file):
                                with open(follower_file, 'r', encoding='utf-8') as f:
                                    content = json.load(f)
                                updated_files.append((follower_file, content))
                            
                            # æ£€æŸ¥å›ç­”æ–‡ä»¶å¤¹
                            answer_dir = os.path.join("D:\\scrapy_zhihu\\data", f"{user_name}_answers")
                            if os.path.exists(answer_dir):
                                answer_files = glob.glob(os.path.join(answer_dir, "*.json"))
                                for file_path in answer_files:
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        content = json.load(f)
                                    updated_files.append((file_path, content))
                            
                            # æ£€æŸ¥æé—®æ–‡ä»¶å¤¹
                            question_dir = os.path.join("D:\\scrapy_zhihu\\data", f"{user_name}_questions")
                            if os.path.exists(question_dir):
                                question_files = glob.glob(os.path.join(question_dir, "*.json"))
                                for file_path in question_files:
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        content = json.load(f)
                                    updated_files.append((file_path, content))
                        elif func_value == 7:
                            st.warning("è‡ªåŠ¨æ›´æ–°æ¨¡å¼å·²å¯åŠ¨ï¼Œå°†æ¯éš”30ç§’æ£€æŸ¥ä¸€æ¬¡æ›´æ–°...")
                            
                            # åˆå§‹åŒ– session_state
                            if 'auto_update_running' not in st.session_state:
                                st.session_state.auto_update_running = True
                            
                            # åˆ›å»ºåœæ­¢æŒ‰é’®
                            if st.button("åœæ­¢è‡ªåŠ¨æ›´æ–°"):
                                st.session_state.auto_update_running = False
                            
                            # åˆå§‹åŒ–ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´
                            last_check_times = {}
                            answer_dir = os.path.join("D:\\scrapy_zhihu\\data", f"{user_name}_answers")
                            question_dir = os.path.join("D:\\scrapy_zhihu\\data", f"{user_name}_questions")
                            
                            # è·å–åˆå§‹æ–‡ä»¶çŠ¶æ€
                            def get_file_status(directory):
                                status = {}
                                if os.path.exists(directory):
                                    for file_path in glob.glob(os.path.join(directory, "*.json")):
                                        status[file_path] = os.path.getmtime(file_path)
                                return status
                            
                            last_answer_status = get_file_status(answer_dir)
                            last_question_status = get_file_status(question_dir)
                            
                            # è‡ªåŠ¨æ›´æ–°å¾ªç¯
                            while st.session_state.auto_update_running:
                                start_time = time.time()
                                
                                # æ‰§è¡Œçˆ¬è™«å‘½ä»¤
                                scrapy_command = [
                                    "scrapy", "crawl", "zhihu",
                                    "-a", f"user_name={user_name}",
                                    "-a", f"func={func_value}"
                                ]
                                
                                try:
                                    result = subprocess.run(scrapy_command, capture_output=True, text=True)
                                    
                                    if result.returncode == 0:
                                        time.sleep(1)  # ç­‰å¾…æ–‡ä»¶å†™å…¥
                                        
                                        # è·å–å½“å‰æ–‡ä»¶çŠ¶æ€
                                        current_answer_status = get_file_status(answer_dir)
                                        current_question_status = get_file_status(question_dir)
                                        
                                        # æ£€æŸ¥å›ç­”æ–‡ä»¶æ›´æ–°
                                        updated_answers = []
                                        for file_path, mtime in current_answer_status.items():
                                            if file_path not in last_answer_status or mtime > last_answer_status[file_path]:
                                                updated_answers.append(file_path)
                                        
                                        # æ£€æŸ¥æé—®æ–‡ä»¶æ›´æ–°
                                        updated_questions = []
                                        for file_path, mtime in current_question_status.items():
                                            if file_path not in last_question_status or mtime > last_question_status[file_path]:
                                                updated_questions.append(file_path)
                                        
                                        # å¦‚æœæœ‰æ›´æ–°
                                        if updated_answers or updated_questions:
                                            st.success("æ£€æµ‹åˆ°å†…å®¹æ›´æ–°ï¼")
                                            
                                            # æ˜¾ç¤ºæ‰€æœ‰å›ç­”æ–‡ä»¶
                                            if os.path.exists(answer_dir):
                                                st.subheader("å›ç­”å†…å®¹ï¼š")
                                                answer_files = glob.glob(os.path.join(answer_dir, "*.json"))
                                                for file_path in answer_files:
                                                    is_updated = file_path in updated_answers
                                                    title = f"{'ğŸ”„ ' if is_updated else ''}å›ç­” - {os.path.basename(file_path)}"
                                                    with st.expander(title):
                                                        with open(file_path, 'r', encoding='utf-8') as f:
                                                            st.json(json.load(f))
                                                        if is_updated:
                                                            st.success("æ­¤æ–‡ä»¶æœ‰æ–°å†…å®¹ï¼")
                                            
                                            # æ˜¾ç¤ºæ‰€æœ‰æé—®æ–‡ä»¶
                                            if os.path.exists(question_dir):
                                                st.subheader("æé—®å†…å®¹ï¼š")
                                                question_files = glob.glob(os.path.join(question_dir, "*.json"))
                                                for file_path in question_files:
                                                    is_updated = file_path in updated_questions
                                                    title = f"{'ğŸ”„ ' if is_updated else ''}æé—® - {os.path.basename(file_path)}"
                                                    with st.expander(title):
                                                        with open(file_path, 'r', encoding='utf-8') as f:
                                                            st.json(json.load(f))
                                                        if is_updated:
                                                            st.success("æ­¤æ–‡ä»¶æœ‰æ–°å†…å®¹ï¼")
                                        
                                        # æ›´æ–°æœ€åæ£€æŸ¥çŠ¶æ€
                                        last_answer_status = current_answer_status
                                        last_question_status = current_question_status
                                    
                                    else:
                                        st.error(f"çˆ¬è™«æ‰§è¡Œå¤±è´¥: {result.stderr}")
                                
                                except Exception as e:
                                    st.error(f"æ‰§è¡Œçˆ¬è™«å‘½ä»¤æ—¶å‡ºé”™: {e}")
                                
                                # ç­‰å¾…30ç§’æˆ–ç›´åˆ°åœæ­¢
                                while time.time() - start_time < 30 and st.session_state.auto_update_running:
                                    time.sleep(1)
                            
                            st.success("è‡ªåŠ¨æ›´æ–°å·²åœæ­¢")
                            # é‡ç½®çŠ¶æ€ä»¥ä¾¿ä¸‹æ¬¡å¯åŠ¨
                            st.session_state.auto_update_running = True

                        # å…¶ä»–åŠŸèƒ½ä¿æŒåŸæœ‰é€»è¾‘
                        else:
                            for i, file_path in enumerate(files_to_check):
                                if file_path:
                                    updated, mtime, content = check_file_update(file_path, last_times[i])
                                    if updated:
                                        updated_files.append((file_path, content))
                        
                        # æ˜¾ç¤ºç»“æœ
                        if updated_files:
                            st.success(f"æˆåŠŸè·å– {len(updated_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
                            for file_path, content in updated_files:
                                # æ ¹æ®æ–‡ä»¶ç±»å‹æ˜¾ç¤ºä¸åŒçš„æ ‡é¢˜
                                if "_answers" in file_path:
                                    title = f"å›ç­”æ•°æ® - {os.path.basename(file_path)}"
                                elif "_questions" in file_path:
                                    title = f"æé—®æ•°æ® - {os.path.basename(file_path)}"
                                elif "_followees" in file_path:
                                    title = "å…³æ³¨è€…åˆ—è¡¨"
                                elif "_followers" in file_path:
                                    title = "ç²‰ä¸åˆ—è¡¨"
                                else:
                                    title = "ç”¨æˆ·åŸºæœ¬ä¿¡æ¯"
                                
                                with st.expander(title):
                                    st.json(content)
                        else:
                            st.warning("çˆ¬è™«æ‰§è¡ŒæˆåŠŸï¼Œä½†æœªæ‰¾åˆ°ç›¸å…³æ–‡ä»¶")
                    else:
                        st.error(f"çˆ¬è™«æ‰§è¡Œå¤±è´¥: {result.stderr}")
            except Exception as e:
                        st.error(f"æ‰§è¡Œçˆ¬è™«å‘½ä»¤æ—¶å‡ºé”™: {e}")
            
            # å…³é—­ Chrome æµè§ˆå™¨
            chrome_process.terminate()
            st.success("Chrome æµè§ˆå™¨å·²å…³é—­ã€‚")
            st.warning("æ³¨æ„ï¼šå¦‚æœçˆ¬å–æ•°æ®ä¸å…¨æˆ–å¤±è´¥ï¼Œè¯·é‡æ–°æ‰§è¡Œè¯¥å‘½ä»¤æˆ– ä½¿ç”¨ (é‡æ–°çˆ¬å–æ‰€æœ‰ä¿¡æ¯)å‘½ä»¤ã€‚")
        except Exception as e:
            st.error(f"å¯åŠ¨ Chrome æµè§ˆå™¨æ—¶å‡ºé”™: {e}")

elif page == "æ•°æ®åˆ†æ":
        st.title("çŸ¥ä¹æ•°æ®åˆ†æ")
        user_name = st.text_input("è¾“å…¥è¦åˆ†æçš„ç”¨æˆ·å", "å»–é›ªå³°")
        if st.button("åˆ†ææ•°æ®"):
            if user_name:
                analyze_data(user_name)
            else:
                st.warning("è¯·è¾“å…¥ç”¨æˆ·å")